{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":121150,"databundleVersionId":15061024,"sourceType":"competition"}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T01:23:46.044422Z","iopub.execute_input":"2025-12-17T01:23:46.044761Z","iopub.status.idle":"2025-12-17T01:23:47.599238Z","shell.execute_reply.started":"2025-12-17T01:23:46.044722Z","shell.execute_reply":"2025-12-17T01:23:47.597687Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T01:32:51.639507Z","iopub.execute_input":"2025-12-17T01:32:51.639927Z","iopub.status.idle":"2025-12-17T01:32:51.656547Z","shell.execute_reply.started":"2025-12-17T01:32:51.639895Z","shell.execute_reply":"2025-12-17T01:32:51.655673Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/deep-past-initiative-machine-translation/train.csv\")\ntest  = pd.read_csv(\"/kaggle/input/deep-past-initiative-machine-translation/test.csv\")\n\nprint(\"Train:\", train.shape)\nprint(\"Test :\", test.shape)\n\ntrain.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T01:33:47.750464Z","iopub.execute_input":"2025-12-17T01:33:47.750823Z","iopub.status.idle":"2025-12-17T01:33:47.865612Z","shell.execute_reply.started":"2025-12-17T01:33:47.750792Z","shell.execute_reply":"2025-12-17T01:33:47.864609Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(train['transliteration'].iloc[0])\nprint(\"----\")\nprint(train['translation'].iloc[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T01:35:22.352245Z","iopub.execute_input":"2025-12-17T01:35:22.352711Z","iopub.status.idle":"2025-12-17T01:35:22.360792Z","shell.execute_reply.started":"2025-12-17T01:35:22.352674Z","shell.execute_reply":"2025-12-17T01:35:22.359309Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sub_map = str.maketrans(\"₀₁₂₃₄₅₆₇₈₉\", \"0123456789\")\n\ndef clean_transliteration(text):\n    if pd.isna(text):\n        return \"\"\n\n    text = text.translate(sub_map)\n\n    # normalize determinatives\n    text = re.sub(r\"\\(d\\)\", \"D_\", text)\n\n    # remove extra spaces\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n\n    return text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T01:41:30.596127Z","iopub.execute_input":"2025-12-17T01:41:30.596551Z","iopub.status.idle":"2025-12-17T01:41:30.603491Z","shell.execute_reply.started":"2025-12-17T01:41:30.596521Z","shell.execute_reply":"2025-12-17T01:41:30.602351Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample = train['transliteration'].iloc[0]\nprint(sample)\nprint(\"----\")\nprint(clean_transliteration(sample))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T01:42:00.881772Z","iopub.execute_input":"2025-12-17T01:42:00.882154Z","iopub.status.idle":"2025-12-17T01:42:00.888210Z","shell.execute_reply.started":"2025-12-17T01:42:00.882123Z","shell.execute_reply":"2025-12-17T01:42:00.887269Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def clean_translation(text):\n    if pd.isna(text):\n        return \"\"\n\n    # normalize quotes\n    text = text.replace(\"“\", '\"').replace(\"”\", '\"')\n    text = text.replace(\"’\", \"'\")\n\n    # normalize repeated punctuation\n    text = re.sub(r\"\\.{2,}\", \".\", text)\n    text = re.sub(r\",{2,}\", \",\", text)\n\n    # normalize whitespace\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n\n    return text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T01:44:46.343003Z","iopub.execute_input":"2025-12-17T01:44:46.343418Z","iopub.status.idle":"2025-12-17T01:44:46.350094Z","shell.execute_reply.started":"2025-12-17T01:44:46.343388Z","shell.execute_reply":"2025-12-17T01:44:46.348982Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_t = train['translation'].iloc[0]\nprint(sample_t)\nprint(\"----\")\nprint(clean_translation(sample_t))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T01:45:06.403576Z","iopub.execute_input":"2025-12-17T01:45:06.403922Z","iopub.status.idle":"2025-12-17T01:45:06.409874Z","shell.execute_reply.started":"2025-12-17T01:45:06.403895Z","shell.execute_reply":"2025-12-17T01:45:06.408946Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# If src/tgt columns already exist, this won't break anything.\n# Create them if missing.\n\nif \"src\" not in train.columns:\n    # If you used clean_src earlier, map it; otherwise create from transliteration (raw)\n    train[\"src\"] = train.get(\"clean_src\", train[\"transliteration\"]).astype(str)\n\nif \"tgt\" not in train.columns:\n    # If you used clean_tgt earlier, map it; otherwise create from translation (raw)\n    train[\"tgt\"] = train.get(\"clean_tgt\", train[\"translation\"]).astype(str)\n\nprint(\"Columns now include src/tgt?\", \"src\" in train.columns, \"tgt\" in train.columns)\nprint(train[[\"src\",\"tgt\"]].head(2))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T01:51:18.797480Z","iopub.execute_input":"2025-12-17T01:51:18.797814Z","iopub.status.idle":"2025-12-17T01:51:18.820066Z","shell.execute_reply.started":"2025-12-17T01:51:18.797786Z","shell.execute_reply":"2025-12-17T01:51:18.818026Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.makedirs(\"/kaggle/working/spm\", exist_ok=True)\n\nsrc_path = \"/kaggle/working/spm/src.txt\"\ntgt_path = \"/kaggle/working/spm/tgt.txt\"\n\nwith open(src_path, \"w\", encoding=\"utf-8\") as f:\n    for s in train[\"src\"].astype(str).tolist():\n        f.write(s.replace(\"\\n\", \" \") + \"\\n\")\n\nwith open(tgt_path, \"w\", encoding=\"utf-8\") as f:\n    for s in train[\"tgt\"].astype(str).tolist():\n        f.write(s.replace(\"\\n\", \" \") + \"\\n\")\n\nprint(\"Wrote:\", src_path, tgt_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T01:51:49.652986Z","iopub.execute_input":"2025-12-17T01:51:49.653310Z","iopub.status.idle":"2025-12-17T01:51:49.675702Z","shell.execute_reply.started":"2025-12-17T01:51:49.653284Z","shell.execute_reply":"2025-12-17T01:51:49.674645Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip -q install sentencepiece\nimport sentencepiece as spm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T01:52:43.676768Z","iopub.execute_input":"2025-12-17T01:52:43.677654Z","iopub.status.idle":"2025-12-17T01:52:47.613498Z","shell.execute_reply.started":"2025-12-17T01:52:43.677615Z","shell.execute_reply":"2025-12-17T01:52:47.612338Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sentencepiece as spm\n\nspm.SentencePieceTrainer.train(\n    input=joint_path,\n    model_prefix=\"/kaggle/working/spm/oa_joint_unigram\",\n    vocab_size=5000,          # <= 5058 (from the error)\n    model_type=\"unigram\",\n    character_coverage=1.0,\n    user_defined_symbols=[\"<gap>\", \"<big_gap>\", \"D_\", \"<morph>\"],\n    hard_vocab_limit=False    # prevents this error in future\n)\n\nprint(\"Trained SentencePiece model with vocab_size=5000.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T01:53:53.943457Z","iopub.execute_input":"2025-12-17T01:53:53.943768Z","iopub.status.idle":"2025-12-17T01:53:54.984598Z","shell.execute_reply.started":"2025-12-17T01:53:53.943743Z","shell.execute_reply":"2025-12-17T01:53:54.983557Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, sentencepiece as spm\n\nprint(\"Files in /kaggle/working/spm:\")\nprint(os.listdir(\"/kaggle/working/spm\"))\n\nsp = spm.SentencePieceProcessor()\nsp.load(\"/kaggle/working/spm/oa_joint_unigram.model\")\n\nprint(\"Tokenizer loaded   Vocab:\", sp.get_piece_size())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T15:49:26.096764Z","iopub.execute_input":"2025-12-17T15:49:26.097359Z","iopub.status.idle":"2025-12-17T15:49:26.114269Z","shell.execute_reply.started":"2025-12-17T15:49:26.097317Z","shell.execute_reply":"2025-12-17T15:49:26.113094Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"SRC pieces:\")\nprint(sp.encode(train[\"src\"].iloc[0], out_type=str)[:60])\n\nprint(\"\\nTGT pieces:\")\nprint(sp.encode(train[\"tgt\"].iloc[0], out_type=str)[:60])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T15:49:39.070300Z","iopub.execute_input":"2025-12-17T15:49:39.070611Z","iopub.status.idle":"2025-12-17T15:49:39.078470Z","shell.execute_reply.started":"2025-12-17T15:49:39.070586Z","shell.execute_reply":"2025-12-17T15:49:39.076701Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import train_test_split\n\ndef encode_with_sp(text):\n    ids = sp.encode(text, out_type=int)\n    return [sp.bos_id()] + ids + [sp.eos_id()]\n\nsrc_ids = [encode_with_sp(s) for s in train[\"src\"].astype(str)]\ntgt_ids = [encode_with_sp(s) for s in train[\"tgt\"].astype(str)]\n\nidx = np.arange(len(train))\ntrain_idx, val_idx = train_test_split(idx, test_size=0.1, random_state=42)\n\nprint(\"Train:\", len(train_idx), \"Val:\", len(val_idx))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T15:50:35.505974Z","iopub.execute_input":"2025-12-17T15:50:35.506432Z","iopub.status.idle":"2025-12-17T15:50:35.891298Z","shell.execute_reply.started":"2025-12-17T15:50:35.506402Z","shell.execute_reply":"2025-12-17T15:50:35.889964Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nPAD_ID = sp.eos_id()\n\nclass MTDataset(Dataset):\n    def __init__(self, src, tgt, idxs):\n        self.src = [src[i] for i in idxs]\n        self.tgt = [tgt[i] for i in idxs]\n    def __len__(self): return len(self.src)\n    def __getitem__(self, i): return self.src[i], self.tgt[i]\n\ndef collate_fn(batch, max_src=512, max_tgt=256):\n    src, tgt = zip(*batch)\n    src = [s[:max_src] for s in src]\n    tgt = [t[:max_tgt] for t in tgt]\n\n    src_len = max(len(s) for s in src)\n    tgt_len = max(len(t) for t in tgt)\n\n    src_pad = torch.full((len(src), src_len), PAD_ID, dtype=torch.long)\n    tgt_pad = torch.full((len(tgt), tgt_len), PAD_ID, dtype=torch.long)\n\n    for i, (s, t) in enumerate(zip(src, tgt)):\n        src_pad[i, :len(s)] = torch.tensor(s)\n        tgt_pad[i, :len(t)] = torch.tensor(t)\n\n    return src_pad, tgt_pad\n\ntrain_loader = DataLoader(MTDataset(src_ids, tgt_ids, train_idx), batch_size=32, shuffle=True, collate_fn=collate_fn)\nval_loader   = DataLoader(MTDataset(src_ids, tgt_ids, val_idx),   batch_size=32, shuffle=False, collate_fn=collate_fn)\n\nprint(\"Batches:\", len(train_loader), len(val_loader))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T15:50:56.382605Z","iopub.execute_input":"2025-12-17T15:50:56.382958Z","iopub.status.idle":"2025-12-17T15:50:56.396793Z","shell.execute_reply.started":"2025-12-17T15:50:56.382929Z","shell.execute_reply":"2025-12-17T15:50:56.395380Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn as nn\nimport math\n\nVOCAB = sp.get_piece_size()\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=4096, dropout=0.1):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout)\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n    def forward(self, x):\n        return self.dropout(x + self.pe[:, :x.size(1)])\n\nclass TinyTransformerMT(nn.Module):\n    def __init__(self, vocab_size, d_model=256, nhead=4, layers=3, dim_ff=1024, dropout=0.1):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, d_model)\n        self.pos = PositionalEncoding(d_model, dropout=dropout)\n        self.tf  = nn.Transformer(\n            d_model=d_model, nhead=nhead,\n            num_encoder_layers=layers, num_decoder_layers=layers,\n            dim_feedforward=dim_ff, dropout=dropout,\n            batch_first=True\n        )\n        self.out = nn.Linear(d_model, vocab_size)\n\n    def forward(self, src_ids, tgt_in_ids):\n        src = self.pos(self.emb(src_ids))\n        tgt = self.pos(self.emb(tgt_in_ids))\n        T = tgt_in_ids.size(1)\n        tgt_mask = torch.triu(torch.ones(T, T, device=src.device), diagonal=1).bool()\n        h = self.tf(src, tgt, tgt_mask=tgt_mask)\n        return self.out(h)\n\nmodel = TinyTransformerMT(VOCAB).to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T15:51:13.477762Z","iopub.execute_input":"2025-12-17T15:51:13.478152Z","iopub.status.idle":"2025-12-17T15:51:13.592689Z","shell.execute_reply.started":"2025-12-17T15:51:13.478121Z","shell.execute_reply":"2025-12-17T15:51:13.591316Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn as nn\nimport math\n\nVOCAB = sp.get_piece_size()\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=4096, dropout=0.1):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout)\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n    def forward(self, x):\n        return self.dropout(x + self.pe[:, :x.size(1)])\n\nclass TinyTransformerMT(nn.Module):\n    def __init__(self, vocab_size, d_model=256, nhead=4, layers=3, dim_ff=1024, dropout=0.1):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, d_model)\n        self.pos = PositionalEncoding(d_model, dropout=dropout)\n        self.tf  = nn.Transformer(\n            d_model=d_model, nhead=nhead,\n            num_encoder_layers=layers, num_decoder_layers=layers,\n            dim_feedforward=dim_ff, dropout=dropout,\n            batch_first=True\n        )\n        self.out = nn.Linear(d_model, vocab_size)\n\n    def forward(self, src_ids, tgt_in_ids):\n        src = self.pos(self.emb(src_ids))\n        tgt = self.pos(self.emb(tgt_in_ids))\n        T = tgt_in_ids.size(1)\n        tgt_mask = torch.triu(torch.ones(T, T, device=src.device), diagonal=1).bool()\n        h = self.tf(src, tgt, tgt_mask=tgt_mask)\n        return self.out(h)\n\nmodel = TinyTransformerMT(VOCAB).to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T15:51:33.541972Z","iopub.execute_input":"2025-12-17T15:51:33.542378Z","iopub.status.idle":"2025-12-17T15:51:33.640632Z","shell.execute_reply.started":"2025-12-17T15:51:33.542347Z","shell.execute_reply":"2025-12-17T15:51:33.639558Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss(ignore_index=PAD_ID)\noptimizer = optim.AdamW(model.parameters(), lr=3e-4)\n\ndef run_epoch(loader, train_mode=True):\n    model.train(train_mode)\n    total = 0.0\n    for src, tgt in loader:\n        src, tgt = src.to(device), tgt.to(device)\n        tgt_in, tgt_out = tgt[:, :-1], tgt[:, 1:]\n\n        logits = model(src, tgt_in)\n        loss = criterion(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n\n        if train_mode:\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n\n        total += loss.item()\n    return total / len(loader)\n\nfor ep in range(3):\n    tr = run_epoch(train_loader, True)\n    va = run_epoch(val_loader, False)\n    print(f\"Epoch {ep+1}: train_loss={tr:.4f} val_loss={va:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T15:51:48.810525Z","iopub.execute_input":"2025-12-17T15:51:48.811665Z","iopub.status.idle":"2025-12-17T16:17:44.349324Z","shell.execute_reply.started":"2025-12-17T15:51:48.811625Z","shell.execute_reply":"2025-12-17T16:17:44.348238Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip -q install sacrebleu\nimport sacrebleu\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T16:18:00.572652Z","iopub.execute_input":"2025-12-17T16:18:00.572988Z","iopub.status.idle":"2025-12-17T16:18:05.216410Z","shell.execute_reply.started":"2025-12-17T16:18:00.572953Z","shell.execute_reply":"2025-12-17T16:18:05.215058Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\n@torch.no_grad()\ndef beam_decode_batch(src, beam_size=5, max_len=128, length_penalty=0.8):\n    model.eval()\n    src = src.to(device)\n\n    # each beam: (seq, logprob)\n    beams = [(torch.full((src.size(0), 1), sp.bos_id(), device=device), torch.zeros(src.size(0), device=device))]\n\n    for _ in range(max_len):\n        new_beams = []\n        for seq, score in beams:\n            logits = model(src, seq)\n            logp = torch.log_softmax(logits[:, -1], dim=-1)\n            topk = torch.topk(logp, beam_size, dim=-1)\n\n            for k in range(beam_size):\n                next_id = topk.indices[:, k:k+1]\n                next_score = score + topk.values[:, k]\n                new_seq = torch.cat([seq, next_id], dim=1)\n                new_beams.append((new_seq, next_score))\n\n        # rank beams by length-penalized avg score\n        def rank_key(item):\n            seq, sc = item\n            lp = (seq.size(1) ** length_penalty)\n            return (sc / lp).mean().item()\n\n        new_beams.sort(key=rank_key, reverse=True)\n        beams = new_beams[:beam_size]\n\n        if all((b[0][:, -1] == sp.eos_id()).all() for b in beams):\n            break\n\n    return beams[0][0]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@torch.no_grad()\ndef eval_on_val(max_batches=10):\n    preds, refs = [], []\n    seen = 0\n    for i, (src, tgt) in enumerate(val_loader):\n        if i >= max_batches:\n            break\n        pred_ids = beam_decode_batch(src, beam_size=5, max_len=128, length_penalty=0.8)\n        for j in range(pred_ids.size(0)):\n            preds.append(sp.decode(pred_ids[j].tolist()))\n            refs.append(train[\"tgt\"].iloc[val_idx[seen]])\n            seen += 1\n\n    bleu = sacrebleu.corpus_bleu(preds, [refs])\n    chrf = sacrebleu.corpus_chrf(preds, [refs], word_order=2)\n    final = (bleu.score * chrf.score) ** 0.5\n\n    print(f\"BLEU  : {bleu.score:.2f}\")\n    print(f\"chrF++: {chrf.score:.2f}\")\n    print(f\"FINAL : {final:.2f}\")\n\neval_on_val(max_batches=10)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q sacrebleu\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T18:52:53.347146Z","iopub.execute_input":"2025-12-25T18:52:53.347511Z","iopub.status.idle":"2025-12-25T18:52:59.697961Z","shell.execute_reply.started":"2025-12-25T18:52:53.347483Z","shell.execute_reply":"2025-12-25T18:52:59.696841Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\nfrom nltk.translate.chrf_score import corpus_chrf\n\ndef compute_metrics(preds, refs):\n    refs_tok = [[r.split()] for r in refs]\n    preds_tok = [p.split() for p in preds]\n\n    bleu = corpus_bleu(refs_tok, preds_tok, smoothing_function=SmoothingFunction().method4)\n    chrf = corpus_chrf(refs, preds)\n\n    return bleu * 100, chrf * 100\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T18:53:22.011056Z","iopub.execute_input":"2025-12-25T18:53:22.011515Z","iopub.status.idle":"2025-12-25T18:53:24.306271Z","shell.execute_reply.started":"2025-12-25T18:53:22.011478Z","shell.execute_reply":"2025-12-25T18:53:24.305201Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, re\nimport pandas as pd\nimport sentencepiece as spm\n\n# 1) Load data again (because kernel resets kill variables)\ntrain = pd.read_csv(\"/kaggle/input/deep-past-initiative-machine-translation/train.csv\")\ntest  = pd.read_csv(\"/kaggle/input/deep-past-initiative-machine-translation/test.csv\")\n\n# 2) Cleaning (same as your proven good version)\nsub_map = str.maketrans(\"₀₁₂₃₄₅₆₇₈₉\", \"0123456789\")\n\ndef clean_transliteration(text):\n    text = str(text).translate(sub_map)\n    text = re.sub(r\"\\(d\\)\", \"D_\", text)\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text\n\ndef clean_translation(text):\n    text = str(text).replace(\"“\", '\"').replace(\"”\", '\"').replace(\"’\", \"'\")\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text\n\ntrain[\"src\"] = train[\"transliteration\"].apply(clean_transliteration)\ntrain[\"tgt\"] = train[\"translation\"].apply(clean_translation)\n\n# 3) Build spm files\nos.makedirs(\"/kaggle/working/spm\", exist_ok=True)\n\nsrc_path   = \"/kaggle/working/spm/src.txt\"\ntgt_path   = \"/kaggle/working/spm/tgt.txt\"\njoint_path = \"/kaggle/working/spm/joint.txt\"\n\nwith open(src_path, \"w\", encoding=\"utf-8\") as f:\n    for s in train[\"src\"]:\n        f.write(s.replace(\"\\n\", \" \") + \"\\n\")\n\nwith open(tgt_path, \"w\", encoding=\"utf-8\") as f:\n    for s in train[\"tgt\"]:\n        f.write(s.replace(\"\\n\", \" \") + \"\\n\")\n\nwith open(joint_path, \"w\", encoding=\"utf-8\") as out:\n    with open(src_path, \"r\", encoding=\"utf-8\") as f:\n        out.write(f.read())\n    with open(tgt_path, \"r\", encoding=\"utf-8\") as f:\n        out.write(f.read())\n\nprint(\" src.txt / tgt.txt / joint.txt created\")\n\n# 4) Train sentencepiece\nspm.SentencePieceTrainer.train(\n    input=joint_path,\n    model_prefix=\"/kaggle/working/spm/oa_joint_unigram\",\n    vocab_size=5000,\n    model_type=\"unigram\",\n    character_coverage=1.0,\n    user_defined_symbols=[\"<gap>\", \"<big_gap>\", \"D_\", \"<morph>\"],\n    hard_vocab_limit=False\n)\nprint(\" SentencePiece trained\")\n\n# 5) Load tokenizer\nsp = spm.SentencePieceProcessor()\nsp.load(\"/kaggle/working/spm/oa_joint_unigram.model\")\nprint(\" Tokenizer loaded | vocab:\", sp.get_piece_size())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T18:58:45.005657Z","iopub.execute_input":"2025-12-25T18:58:45.006027Z","iopub.status.idle":"2025-12-25T18:58:46.214388Z","shell.execute_reply.started":"2025-12-25T18:58:45.005996Z","shell.execute_reply":"2025-12-25T18:58:46.213151Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nPAD_ID = sp.eos_id()\n\ndef encode_with_sp(text):\n    ids = sp.encode(text, out_type=int)\n    return [sp.bos_id()] + ids + [sp.eos_id()]\n\nsrc_ids = [encode_with_sp(s) for s in train[\"src\"].astype(str)]\ntgt_ids = [encode_with_sp(s) for s in train[\"tgt\"].astype(str)]\n\nidx = np.arange(len(train))\ntrain_idx, val_idx = train_test_split(idx, test_size=0.1, random_state=42)\n\nclass MTDataset(Dataset):\n    def __init__(self, src, tgt, idxs):\n        self.src = [src[i] for i in idxs]\n        self.tgt = [tgt[i] for i in idxs]\n    def __len__(self): return len(self.src)\n    def __getitem__(self, i): return self.src[i], self.tgt[i]\n\ndef collate_fn(batch, max_src=512, max_tgt=256):\n    src, tgt = zip(*batch)\n    src = [s[:max_src] for s in src]\n    tgt = [t[:max_tgt] for t in tgt]\n\n    src_len = max(len(s) for s in src)\n    tgt_len = max(len(t) for t in tgt)\n\n    src_pad = torch.full((len(src), src_len), PAD_ID, dtype=torch.long)\n    tgt_pad = torch.full((len(tgt), tgt_len), PAD_ID, dtype=torch.long)\n\n    for i, (s, t) in enumerate(zip(src, tgt)):\n        src_pad[i, :len(s)] = torch.tensor(s)\n        tgt_pad[i, :len(t)] = torch.tensor(t)\n\n    return src_pad, tgt_pad\n\ntrain_loader = DataLoader(MTDataset(src_ids, tgt_ids, train_idx), batch_size=32, shuffle=True, collate_fn=collate_fn)\nval_loader   = DataLoader(MTDataset(src_ids, tgt_ids, val_idx),   batch_size=32, shuffle=False, collate_fn=collate_fn)\n\nprint(\" loaders ready | train batches:\", len(train_loader), \"val batches:\", len(val_loader))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T18:59:28.751269Z","iopub.execute_input":"2025-12-25T18:59:28.751682Z","iopub.status.idle":"2025-12-25T18:59:29.163523Z","shell.execute_reply.started":"2025-12-25T18:59:28.751651Z","shell.execute_reply":"2025-12-25T18:59:29.162480Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn as nn\nimport math\n\nVOCAB = sp.get_piece_size()\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=4096, dropout=0.1):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout)\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n    def forward(self, x):\n        return self.dropout(x + self.pe[:, :x.size(1)])\n\nclass TinyTransformerMT(nn.Module):\n    def __init__(self, vocab_size, d_model=256, nhead=4, layers=3, dim_ff=1024, dropout=0.1):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, d_model)\n        self.pos = PositionalEncoding(d_model, dropout=dropout)\n        self.tf  = nn.Transformer(\n            d_model=d_model, nhead=nhead,\n            num_encoder_layers=layers, num_decoder_layers=layers,\n            dim_feedforward=dim_ff, dropout=dropout,\n            batch_first=True\n        )\n        self.out = nn.Linear(d_model, vocab_size)\n\n    def forward(self, src_ids, tgt_in_ids):\n        src = self.pos(self.emb(src_ids))\n        tgt = self.pos(self.emb(tgt_in_ids))\n        T = tgt_in_ids.size(1)\n        tgt_mask = torch.triu(torch.ones(T, T, device=src.device), diagonal=1).bool()\n        h = self.tf(src, tgt, tgt_mask=tgt_mask)\n        return self.out(h)\n\nmodel = TinyTransformerMT(VOCAB).to(device)\nprint(\" model ready on\", device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T18:59:45.019494Z","iopub.execute_input":"2025-12-25T18:59:45.019881Z","iopub.status.idle":"2025-12-25T18:59:45.241263Z","shell.execute_reply.started":"2025-12-25T18:59:45.019849Z","shell.execute_reply":"2025-12-25T18:59:45.240132Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss(ignore_index=PAD_ID)\noptimizer = optim.AdamW(model.parameters(), lr=3e-4)\n\ndef run_epoch(loader, train_mode=True):\n    model.train(train_mode)\n    total = 0.0\n    for src, tgt in loader:\n        src, tgt = src.to(device), tgt.to(device)\n        tgt_in, tgt_out = tgt[:, :-1], tgt[:, 1:]\n\n        logits = model(src, tgt_in)\n        loss = criterion(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n\n        if train_mode:\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n\n        total += loss.item()\n    return total / len(loader)\n\nfor ep in range(3):\n    tr = run_epoch(train_loader, True)\n    va = run_epoch(val_loader, False)\n    print(f\"Epoch {ep+1}: train_loss={tr:.4f} val_loss={va:.4f}\")\n\n# optional: save weights so next restart doesn't waste time\ntorch.save(model.state_dict(), \"/kaggle/working/model_baseline.pt\")\nprint(\" saved: /kaggle/working/model_baseline.pt\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T18:59:58.091696Z","iopub.execute_input":"2025-12-25T18:59:58.092062Z","iopub.status.idle":"2025-12-25T19:27:11.444830Z","shell.execute_reply.started":"2025-12-25T18:59:58.092032Z","shell.execute_reply":"2025-12-25T19:27:11.443326Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip -q install sacrebleu\nimport sacrebleu\nimport torch\n\n@torch.no_grad()\ndef beam_decode_batch(src, beam_size=5, max_len=160, length_penalty=0.8):\n    model.eval()\n    src = src.to(device)\n\n    beams = [(torch.full((src.size(0), 1), sp.bos_id(), device=device), torch.zeros(src.size(0), device=device))]\n\n    for _ in range(max_len):\n        new_beams = []\n        for seq, score in beams:\n            logits = model(src, seq)\n            logp = torch.log_softmax(logits[:, -1], dim=-1)\n            topk = torch.topk(logp, beam_size, dim=-1)\n\n            for k in range(beam_size):\n                next_id = topk.indices[:, k:k+1]\n                next_score = score + topk.values[:, k]\n                new_seq = torch.cat([seq, next_id], dim=1)\n                new_beams.append((new_seq, next_score))\n\n        def rank(item):\n            seq, sc = item\n            lp = (seq.size(1) ** length_penalty)\n            return (sc / lp).mean().item()\n\n        new_beams.sort(key=rank, reverse=True)\n        beams = new_beams[:beam_size]\n\n        if all((b[0][:, -1] == sp.eos_id()).all() for b in beams):\n            break\n\n    return beams[0][0]\n\n@torch.no_grad()\ndef eval_val(max_batches=20, beam_size=5, lp=0.8):\n    preds, refs = [], []\n    seen = 0\n    for b, (src, tgt) in enumerate(val_loader):\n        if b >= max_batches:\n            break\n        pred_ids = beam_decode_batch(src, beam_size=beam_size, length_penalty=lp)\n        for i in range(pred_ids.size(0)):\n            preds.append(sp.decode(pred_ids[i].tolist()))\n            refs.append(train[\"tgt\"].iloc[val_idx[seen]])\n            seen += 1\n\n    bleu = sacrebleu.corpus_bleu(preds, [refs]).score\n    chrf = sacrebleu.corpus_chrf(preds, [refs], word_order=2).score\n    final = (bleu * chrf) ** 0.5\n    print(f\"beam={beam_size} lp={lp} | BLEU={bleu:.2f} chrF++={chrf:.2f} FINAL={final:.2f}\")\n\neval_val(max_batches=20, beam_size=5, lp=0.8)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-25T22:55:11.211Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\nimport os, re, pandas as pd, sentencepiece as spm, torch\n\n# 1) Load data\ntrain = pd.read_csv(\"/kaggle/input/deep-past-initiative-machine-translation/train.csv\")\n\n# 2) Clean functions\nsub_map = str.maketrans(\"₀₁₂₃₄₅₆₇₈₉\", \"0123456789\")\n\ndef clean_transliteration(t):\n    t = str(t).translate(sub_map)\n    t = re.sub(r\"\\(d\\)\", \"D_\", t)\n    t = re.sub(r\"\\s+\", \" \", t).strip()\n    return t\n\ndef clean_translation(t):\n    t = str(t).replace(\"“\", '\"').replace(\"”\", '\"').replace(\"’\", \"'\")\n    t = re.sub(r\"\\s+\", \" \", t).strip()\n    return t\n\ntrain[\"src\"] = train[\"transliteration\"].apply(clean_transliteration)\ntrain[\"tgt\"] = train[\"translation\"].apply(clean_translation)\n\n# 3) Build SPM files\nos.makedirs(\"/kaggle/working/spm\", exist_ok=True)\n\nsrc_path   = \"/kaggle/working/spm/src.txt\"\ntgt_path   = \"/kaggle/working/spm/tgt.txt\"\njoint_path = \"/kaggle/working/spm/joint.txt\"\n\nwith open(src_path, \"w\", encoding=\"utf-8\") as f:\n    f.write(\"\\n\".join(train[\"src\"].tolist()))\n\nwith open(tgt_path, \"w\", encoding=\"utf-8\") as f:\n    f.write(\"\\n\".join(train[\"tgt\"].tolist()))\n\nwith open(joint_path, \"w\", encoding=\"utf-8\") as f:\n    f.write(open(src_path).read())\n    f.write(\"\\n\")\n    f.write(open(tgt_path).read())\n\n# 4) Train SentencePiece (always safe)\nspm.SentencePieceTrainer.train(\n    input=joint_path,\n    model_prefix=\"/kaggle/working/spm/oa_joint_unigram\",\n    vocab_size=5000,\n    model_type=\"unigram\",\n    character_coverage=1.0,\n    user_defined_symbols=[\"<gap>\", \"<big_gap>\", \"D_\", \"<morph>\"],\n    hard_vocab_limit=False\n)\n\n# 5) Load tokenizer\nsp = spm.SentencePieceProcessor()\nsp.load(\"/kaggle/working/spm/oa_joint_unigram.model\")\n\nPAD_ID = sp.eos_id()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(\" BOOTSTRAP DONE\")\nprint(\"Vocab:\", sp.get_piece_size(), \"| PAD_ID:\", PAD_ID, \"| Device:\", device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T23:54:44.966375Z","iopub.execute_input":"2025-12-27T23:54:44.966654Z","iopub.status.idle":"2025-12-27T23:54:45.775609Z","shell.execute_reply.started":"2025-12-27T23:54:44.966636Z","shell.execute_reply":"2025-12-27T23:54:45.774570Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import train_test_split\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\ndef encode_with_sp(text):\n    ids = sp.encode(str(text), out_type=int)\n    return [sp.bos_id()] + ids + [sp.eos_id()]\n\nsrc_ids = [encode_with_sp(s) for s in train[\"src\"]]\ntgt_ids = [encode_with_sp(s) for s in train[\"tgt\"]]\n\nidx = np.arange(len(train))\ntrain_idx, val_idx = train_test_split(idx, test_size=0.1, random_state=42)\n\nclass MTDataset(Dataset):\n    def __init__(self, src, tgt, idxs):\n        self.src = [src[i] for i in idxs]\n        self.tgt = [tgt[i] for i in idxs]\n    def __len__(self): return len(self.src)\n    def __getitem__(self, i): return self.src[i], self.tgt[i]\n\ndef collate_fn(batch, max_src=512, max_tgt=256):\n    src, tgt = zip(*batch)\n    src = [s[:max_src] for s in src]\n    tgt = [t[:max_tgt] for t in tgt]\n\n    src_len = max(len(s) for s in src)\n    tgt_len = max(len(t) for t in tgt)\n\n    src_pad = torch.full((len(src), src_len), PAD_ID, dtype=torch.long)\n    tgt_pad = torch.full((len(tgt), tgt_len), PAD_ID, dtype=torch.long)\n\n    for i, (s, t) in enumerate(zip(src, tgt)):\n        src_pad[i, :len(s)] = torch.tensor(s)\n        tgt_pad[i, :len(t)] = torch.tensor(t)\n    return src_pad, tgt_pad\n\ntrain_loader = DataLoader(MTDataset(src_ids, tgt_ids, train_idx), batch_size=32, shuffle=True, collate_fn=collate_fn)\nval_loader   = DataLoader(MTDataset(src_ids, tgt_ids, val_idx),   batch_size=32, shuffle=False, collate_fn=collate_fn)\n\nprint(\" loaders ready | train batches:\", len(train_loader), \"val batches:\", len(val_loader))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T23:56:09.214438Z","iopub.execute_input":"2025-12-27T23:56:09.214808Z","iopub.status.idle":"2025-12-27T23:56:09.486948Z","shell.execute_reply.started":"2025-12-27T23:56:09.214778Z","shell.execute_reply":"2025-12-27T23:56:09.485946Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn as nn\nimport math\n\nVOCAB = sp.get_piece_size()\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=4096, dropout=0.1):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout)\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n    def forward(self, x):\n        return self.dropout(x + self.pe[:, :x.size(1)])\n\nclass TinyTransformerMT(nn.Module):\n    def __init__(self, vocab_size, d_model=256, nhead=4, layers=3, dim_ff=1024, dropout=0.1):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, d_model)\n        self.pos = PositionalEncoding(d_model, dropout=dropout)\n        self.tf  = nn.Transformer(\n            d_model=d_model, nhead=nhead,\n            num_encoder_layers=layers, num_decoder_layers=layers,\n            dim_feedforward=dim_ff, dropout=dropout,\n            batch_first=True\n        )\n        self.out = nn.Linear(d_model, vocab_size)\n\n    def forward(self, src_ids, tgt_in_ids):\n        src = self.pos(self.emb(src_ids))\n        tgt = self.pos(self.emb(tgt_in_ids))\n        T = tgt_in_ids.size(1)\n        tgt_mask = torch.triu(torch.ones(T, T, device=src.device), diagonal=1).bool()\n        h = self.tf(src, tgt, tgt_mask=tgt_mask)\n        return self.out(h)\n\nmodel = TinyTransformerMT(VOCAB).to(device)\nprint(\" model ready on\", device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T23:56:27.116764Z","iopub.execute_input":"2025-12-27T23:56:27.117131Z","iopub.status.idle":"2025-12-27T23:56:27.324736Z","shell.execute_reply.started":"2025-12-27T23:56:27.117102Z","shell.execute_reply":"2025-12-27T23:56:27.323425Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss(ignore_index=PAD_ID)\noptimizer = optim.AdamW(model.parameters(), lr=3e-4)\n\ndef run_epoch(loader, train_mode=True):\n    model.train(train_mode)\n    total = 0.0\n    for src, tgt in loader:\n        src, tgt = src.to(device), tgt.to(device)\n        tgt_in, tgt_out = tgt[:, :-1], tgt[:, 1:]\n\n        logits = model(src, tgt_in)\n        loss = criterion(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n\n        if train_mode:\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n\n        total += loss.item()\n    return total / len(loader)\n\nfor ep in range(2):\n    tr = run_epoch(train_loader, True)\n    va = run_epoch(val_loader, False)\n    print(f\"Epoch {ep+1}: train_loss={tr:.4f} val_loss={va:.4f}\")\n\nprint(\" training done\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T23:56:49.979259Z","iopub.execute_input":"2025-12-27T23:56:49.979508Z","iopub.status.idle":"2025-12-28T00:10:40.259989Z","shell.execute_reply.started":"2025-12-27T23:56:49.979487Z","shell.execute_reply":"2025-12-28T00:10:40.258132Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip -q install sacrebleu\nimport sacrebleu\nimport torch\n\n@torch.no_grad()\ndef beam_decode_batch(src, beam_size=5, max_len=160, length_penalty=0.8):\n    model.eval()\n    src = src.to(device)\n\n    beams = [(torch.full((src.size(0), 1), sp.bos_id(), device=device), torch.zeros(src.size(0), device=device))]\n\n    for _ in range(max_len):\n        new_beams = []\n        for seq, score in beams:\n            logits = model(src, seq)\n            logp = torch.log_softmax(logits[:, -1], dim=-1)\n            topk = torch.topk(logp, beam_size, dim=-1)\n\n            for k in range(beam_size):\n                next_id = topk.indices[:, k:k+1]\n                next_score = score + topk.values[:, k]\n                new_seq = torch.cat([seq, next_id], dim=1)\n                new_beams.append((new_seq, next_score))\n\n        def rank(item):\n            seq, sc = item\n            lp = (seq.size(1) ** length_penalty)\n            return (sc / lp).mean().item()\n\n        new_beams.sort(key=rank, reverse=True)\n        beams = new_beams[:beam_size]\n\n        if all((b[0][:, -1] == sp.eos_id()).all() for b in beams):\n            break\n\n    return beams[0][0]\n\n@torch.no_grad()\ndef eval_val(max_batches=20, beam_size=5, lp=0.8):\n    preds, refs = [], []\n    seen = 0\n    for b, (src, tgt) in enumerate(val_loader):\n        if b >= max_batches:\n            break\n        pred_ids = beam_decode_batch(src, beam_size=beam_size, length_penalty=lp)\n        for i in range(pred_ids.size(0)):\n            preds.append(sp.decode(pred_ids[i].tolist()))\n            refs.append(train[\"tgt\"].iloc[val_idx[seen]])\n            seen += 1\n\n    bleu = sacrebleu.corpus_bleu(preds, [refs]).score\n    chrf = sacrebleu.corpus_chrf(preds, [refs], word_order=2).score\n    final = (bleu * chrf) ** 0.5\n    print(f\"[BASELINE] beam={beam_size} lp={lp} | BLEU={bleu:.2f} chrF++={chrf:.2f} FINAL={final:.2f}\")\n    return bleu, chrf, final\n\neval_val(max_batches=20, beam_size=5, lp=0.8)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T00:43:25.694941Z","iopub.execute_input":"2025-12-28T00:43:25.696461Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport re\n\nlex = pd.read_csv(\"/kaggle/input/deep-past-initiative-machine-translation/OA_Lexicon_eBL.csv\")\nprint(\"Lexicon shape:\", lex.shape)\nprint(\"Columns:\", list(lex.columns))\n\nlex.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T23:18:52.211263Z","iopub.execute_input":"2025-12-28T23:18:52.211632Z","iopub.status.idle":"2025-12-28T23:18:53.807552Z","shell.execute_reply.started":"2025-12-28T23:18:52.211600Z","shell.execute_reply":"2025-12-28T23:18:53.806550Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cols = [c.lower() for c in lex.columns]\nlex.columns = cols\n\n# best-guess columns\nsrc_col = None\ntgt_col = None\nfor c in cols:\n    if src_col is None and any(k in c for k in [\"translit\", \"form\", \"lemma\", \"akk\"]):\n        src_col = c\n    if tgt_col is None and any(k in c for k in [\"normal\", \"english\", \"name\", \"norm\"]):\n        tgt_col = c\n\nprint(\"Using columns:\", src_col, \"->\", tgt_col)\n\nlex_map = {}\nif src_col and tgt_col:\n    tmp = lex[[src_col, tgt_col]].dropna()\n    for s, t in tmp.values:\n        s = str(s).strip()\n        t = str(t).strip()\n        if len(s) >= 2 and len(t) >= 2:\n            lex_map[s] = t\n\nprint(\"Lexicon map size:\", len(lex_map))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T23:19:37.582284Z","iopub.execute_input":"2025-12-28T23:19:37.582664Z","iopub.status.idle":"2025-12-28T23:19:37.699132Z","shell.execute_reply.started":"2025-12-28T23:19:37.582631Z","shell.execute_reply":"2025-12-28T23:19:37.698154Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# take top-K keys (speed)\nLEX_K = 3000\nlex_keys = sorted(list(lex_map.keys()), key=len, reverse=True)[:LEX_K]\n\nlex_re = re.compile(r\"\\b(\" + \"|\".join(map(re.escape, lex_keys)) + r\")\\b\")\n\ndef lexicon_postprocess(text: str) -> str:\n    return lex_re.sub(lambda m: lex_map.get(m.group(1), m.group(1)), text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T23:19:53.451753Z","iopub.execute_input":"2025-12-28T23:19:53.452516Z","iopub.status.idle":"2025-12-28T23:19:53.474921Z","shell.execute_reply.started":"2025-12-28T23:19:53.452477Z","shell.execute_reply":"2025-12-28T23:19:53.473923Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\n@torch.no_grad()\ndef sample_decode_batch(src, max_len=160, temperature=1.0, topk=40):\n    model.eval()\n    src = src.to(device)\n    ys = torch.full((src.size(0), 1), sp.bos_id(), device=device)\n\n    for _ in range(max_len):\n        logits = model(src, ys)[:, -1, :] / temperature\n        probs = torch.softmax(logits, dim=-1)\n\n        topv, topi = torch.topk(probs, k=min(topk, probs.size(-1)), dim=-1)\n        next_id = topi.gather(1, torch.multinomial(topv, 1))\n        ys = torch.cat([ys, next_id], dim=1)\n\n        if (next_id == sp.eos_id()).all():\n            break\n    return ys\n\ndef rerank_score(s: str) -> float:\n    s = s.strip()\n    if len(s) < 8:\n        return -1e9\n    # reward reasonable length, penalize ugly spacing\n    penalty = 0\n    penalty += 10 * s.count(\"  \")\n    penalty += 5  * (s.count(\" ,\") + s.count(\" .\") + s.count(\" :\"))\n    penalty += 5  * (s.count(\"..\") + s.count(\",,\"))\n    return len(s) - penalty\n\n@torch.no_grad()\ndef nbest_decode_rerank(src, beam_size=5, lp=0.8, n_samples=4):\n    # candidate 0: beam\n    beam_ids = beam_decode_batch(src, beam_size=beam_size, max_len=160, length_penalty=lp)\n    cand_lists = [[sp.decode(beam_ids[i].tolist()) for i in range(beam_ids.size(0))]]\n\n    # candidates 1..n: sampling\n    for _ in range(n_samples):\n        samp_ids = sample_decode_batch(src, max_len=160, temperature=1.0, topk=40)\n        cand_lists.append([sp.decode(samp_ids[i].tolist()) for i in range(samp_ids.size(0))])\n\n    final = []\n    B = len(cand_lists[0])\n    for i in range(B):\n        options = [cand_lists[k][i] for k in range(len(cand_lists))]\n        # apply lexicon correction\n        options = [lexicon_postprocess(o) for o in options]\n        options.sort(key=rerank_score, reverse=True)\n        final.append(options[0])\n    return final\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T23:20:04.762114Z","iopub.execute_input":"2025-12-28T23:20:04.762467Z","iopub.status.idle":"2025-12-28T23:20:09.932877Z","shell.execute_reply.started":"2025-12-28T23:20:04.762436Z","shell.execute_reply":"2025-12-28T23:20:09.931514Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip -q install sacrebleu\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T23:21:21.320193Z","iopub.execute_input":"2025-12-28T23:21:21.320575Z","iopub.status.idle":"2025-12-28T23:21:27.444899Z","shell.execute_reply.started":"2025-12-28T23:21:21.320543Z","shell.execute_reply":"2025-12-28T23:21:27.443668Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sacrebleu\nprint(\"sacrebleu version:\", sacrebleu.__version__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T23:21:36.784289Z","iopub.execute_input":"2025-12-28T23:21:36.784649Z","iopub.status.idle":"2025-12-28T23:21:36.904656Z","shell.execute_reply.started":"2025-12-28T23:21:36.784611Z","shell.execute_reply":"2025-12-28T23:21:36.903515Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\nfrom nltk.translate.chrf_score import corpus_chrf\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T23:21:44.596189Z","iopub.execute_input":"2025-12-28T23:21:44.596764Z","iopub.status.idle":"2025-12-28T23:21:47.245962Z","shell.execute_reply.started":"2025-12-28T23:21:44.596733Z","shell.execute_reply":"2025-12-28T23:21:47.244985Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_metrics(preds, refs):\n    # BLEU\n    refs_tok = [[r.split()] for r in refs]\n    preds_tok = [p.split() for p in preds]\n\n    bleu = corpus_bleu(\n        refs_tok,\n        preds_tok,\n        smoothing_function=SmoothingFunction().method4\n    ) * 100\n\n    # chrF++\n    chrf = corpus_chrf(refs, preds, beta=2) * 100\n\n    final = (bleu * chrf) ** 0.5\n    return bleu, chrf, final\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T23:22:02.763435Z","iopub.execute_input":"2025-12-28T23:22:02.763775Z","iopub.status.idle":"2025-12-28T23:22:02.770471Z","shell.execute_reply.started":"2025-12-28T23:22:02.763745Z","shell.execute_reply":"2025-12-28T23:22:02.769347Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@torch.no_grad()\ndef eval_val_boosted_fallback(max_batches=20, beam_size=5, lp=0.8, n_samples=4):\n    preds, refs = [], []\n    seen = 0\n\n    for b, (src, tgt) in enumerate(val_loader):\n        if b >= max_batches:\n            break\n\n        out_txt = nbest_decode_rerank(src, beam_size=beam_size, lp=lp, n_samples=n_samples)\n\n        for i in range(len(out_txt)):\n            preds.append(out_txt[i])\n            refs.append(train[\"tgt\"].iloc[val_idx[seen]])\n            seen += 1\n\n    bleu, chrf, final = compute_metrics(preds, refs)\n    print(f\"[BOOSTED-FALLBACK] beam={beam_size} lp={lp} n_samples={n_samples} | BLEU={bleu:.2f} chrF++={chrf:.2f} FINAL={final:.2f}\")\n    return bleu, chrf, final\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T23:22:12.102980Z","iopub.execute_input":"2025-12-28T23:22:12.104053Z","iopub.status.idle":"2025-12-28T23:22:12.111512Z","shell.execute_reply.started":"2025-12-28T23:22:12.104015Z","shell.execute_reply":"2025-12-28T23:22:12.110281Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport re\n\n# Reload train data\ntrain = pd.read_csv(\"/kaggle/input/deep-past-initiative-machine-translation/train.csv\")\n\n# Recreate cleaned columns (must match bootstrap)\nsub_map = str.maketrans(\"₀₁₂₃₄₅₆₇₈₉\", \"0123456789\")\n\ndef clean_transliteration(t):\n    t = str(t).translate(sub_map)\n    t = re.sub(r\"\\(d\\)\", \"D_\", t)\n    t = re.sub(r\"\\s+\", \" \", t).strip()\n    return t\n\ndef clean_translation(t):\n    t = str(t).replace(\"“\", '\"').replace(\"”\", '\"').replace(\"’\", \"'\")\n    t = re.sub(r\"\\s+\", \" \", t).strip()\n    return t\n\ntrain[\"src\"] = train[\"transliteration\"].apply(clean_transliteration)\ntrain[\"tgt\"] = train[\"translation\"].apply(clean_translation)\n\nprint(\" train reloaded:\", train.shape)\nprint(train[[\"src\",\"tgt\"]].head(1))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T23:24:32.851353Z","iopub.execute_input":"2025-12-28T23:24:32.851710Z","iopub.status.idle":"2025-12-28T23:24:33.126888Z","shell.execute_reply.started":"2025-12-28T23:24:32.851679Z","shell.execute_reply":"2025-12-28T23:24:33.125878Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, re, pandas as pd, sentencepiece as spm\n\n# Reload data (safe)\ntrain = pd.read_csv(\"/kaggle/input/deep-past-initiative-machine-translation/train.csv\")\n\nsub_map = str.maketrans(\"₀₁₂₃₄₅₆₇₈₉\", \"0123456789\")\n\ndef clean_transliteration(t):\n    t = str(t).translate(sub_map)\n    t = re.sub(r\"\\(d\\)\", \"D_\", t)\n    t = re.sub(r\"\\s+\", \" \", t).strip()\n    return t\n\ndef clean_translation(t):\n    t = str(t).replace(\"“\", '\"').replace(\"”\", '\"').replace(\"’\", \"'\")\n    t = re.sub(r\"\\s+\", \" \", t).strip()\n    return t\n\ntrain[\"src\"] = train[\"transliteration\"].apply(clean_transliteration)\ntrain[\"tgt\"] = train[\"translation\"].apply(clean_translation)\n\n# Build SPM files\nos.makedirs(\"/kaggle/working/spm\", exist_ok=True)\n\nsrc_path   = \"/kaggle/working/spm/src.txt\"\ntgt_path   = \"/kaggle/working/spm/tgt.txt\"\njoint_path = \"/kaggle/working/spm/joint.txt\"\n\nwith open(src_path, \"w\", encoding=\"utf-8\") as f:\n    f.write(\"\\n\".join(train[\"src\"].tolist()))\n\nwith open(tgt_path, \"w\", encoding=\"utf-8\") as f:\n    f.write(\"\\n\".join(train[\"tgt\"].tolist()))\n\nwith open(joint_path, \"w\", encoding=\"utf-8\") as f:\n    f.write(open(src_path).read() + \"\\n\" + open(tgt_path).read())\n\n# Train SentencePiece\nspm.SentencePieceTrainer.train(\n    input=joint_path,\n    model_prefix=\"/kaggle/working/spm/oa_joint_unigram\",\n    vocab_size=5000,\n    model_type=\"unigram\",\n    character_coverage=1.0,\n    user_defined_symbols=[\"<gap>\", \"<big_gap>\", \"D_\", \"<morph>\"],\n    hard_vocab_limit=False\n)\n\n# Load tokenizer\nsp = spm.SentencePieceProcessor()\nsp.load(\"/kaggle/working/spm/oa_joint_unigram.model\")\n\nprint(\" sp loaded | vocab:\", sp.get_piece_size())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T23:25:53.612627Z","iopub.execute_input":"2025-12-28T23:25:53.612995Z","iopub.status.idle":"2025-12-28T23:25:54.859665Z","shell.execute_reply.started":"2025-12-28T23:25:53.612963Z","shell.execute_reply":"2025-12-28T23:25:54.858494Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def encode_with_sp(text):\n    ids = sp.encode(str(text), out_type=int)\n    return [sp.bos_id()] + ids + [sp.eos_id()]\n\nprint(\"Encoder ready \")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T23:26:08.287714Z","iopub.execute_input":"2025-12-28T23:26:08.288103Z","iopub.status.idle":"2025-12-28T23:26:08.293993Z","shell.execute_reply.started":"2025-12-28T23:26:08.288072Z","shell.execute_reply":"2025-12-28T23:26:08.292961Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"src_ids = [encode_with_sp(s) for s in train[\"src\"]]\ntgt_ids = [encode_with_sp(s) for s in train[\"tgt\"]]\n\nprint(\"Encoded:\", len(src_ids), len(tgt_ids))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T23:26:17.693341Z","iopub.execute_input":"2025-12-28T23:26:17.693678Z","iopub.status.idle":"2025-12-28T23:26:18.077914Z","shell.execute_reply.started":"2025-12-28T23:26:17.693652Z","shell.execute_reply":"2025-12-28T23:26:18.076944Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, re, pandas as pd, sentencepiece as spm, torch\n\n# 1) Load data\ntrain = pd.read_csv(\"/kaggle/input/deep-past-initiative-machine-translation/train.csv\")\n\nsub_map = str.maketrans(\"₀₁₂₃₄₅₆₇₈₉\", \"0123456789\")\n\ndef clean_transliteration(t):\n    t = str(t).translate(sub_map)\n    t = re.sub(r\"\\(d\\)\", \"D_\", t)\n    t = re.sub(r\"\\s+\", \" \", t).strip()\n    return t\n\ndef clean_translation(t):\n    t = str(t).replace(\"“\", '\"').replace(\"”\", '\"').replace(\"’\", \"'\")\n    t = re.sub(r\"\\s+\", \" \", t).strip()\n    return t\n\ntrain[\"src\"] = train[\"transliteration\"].apply(clean_transliteration)\ntrain[\"tgt\"] = train[\"translation\"].apply(clean_translation)\n\n# 2) Build SentencePiece input\nos.makedirs(\"/kaggle/working/spm\", exist_ok=True)\n\nsrc_path   = \"/kaggle/working/spm/src.txt\"\ntgt_path   = \"/kaggle/working/spm/tgt.txt\"\njoint_path = \"/kaggle/working/spm/joint.txt\"\n\nwith open(src_path, \"w\", encoding=\"utf-8\") as f:\n    f.write(\"\\n\".join(train[\"src\"].tolist()))\nwith open(tgt_path, \"w\", encoding=\"utf-8\") as f:\n    f.write(\"\\n\".join(train[\"tgt\"].tolist()))\nwith open(joint_path, \"w\", encoding=\"utf-8\") as f:\n    f.write(open(src_path).read() + \"\\n\" + open(tgt_path).read())\n\n# 3) Train + load SentencePiece\nspm.SentencePieceTrainer.train(\n    input=joint_path,\n    model_prefix=\"/kaggle/working/spm/oa_joint_unigram\",\n    vocab_size=5000,\n    model_type=\"unigram\",\n    character_coverage=1.0,\n    user_defined_symbols=[\"<gap>\", \"<big_gap>\", \"D_\", \"<morph>\"],\n    hard_vocab_limit=False\n)\n\nsp = spm.SentencePieceProcessor()\nsp.load(\"/kaggle/working/spm/oa_joint_unigram.model\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nPAD_ID = sp.eos_id()\n\nprint(\" CELL A DONE | sp loaded | PAD_ID:\", PAD_ID)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T19:48:49.770752Z","iopub.execute_input":"2025-12-30T19:48:49.771101Z","iopub.status.idle":"2025-12-30T19:48:51.111004Z","shell.execute_reply.started":"2025-12-30T19:48:49.771071Z","shell.execute_reply":"2025-12-30T19:48:51.109719Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def encode_with_sp(text):\n    ids = sp.encode(str(text), out_type=int)\n    return [sp.bos_id()] + ids + [sp.eos_id()]\n\nprint(\" encoder ready\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T19:49:15.827396Z","iopub.execute_input":"2025-12-30T19:49:15.827752Z","iopub.status.idle":"2025-12-30T19:49:15.833974Z","shell.execute_reply.started":"2025-12-30T19:49:15.827721Z","shell.execute_reply":"2025-12-30T19:49:15.832846Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"src_ids = [encode_with_sp(s) for s in train[\"src\"]]\ntgt_ids = [encode_with_sp(s) for s in train[\"tgt\"]]\n\nprint(\"Encoded:\", len(src_ids), len(tgt_ids))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T19:49:23.478435Z","iopub.execute_input":"2025-12-30T19:49:23.478820Z","iopub.status.idle":"2025-12-30T19:49:23.881333Z","shell.execute_reply.started":"2025-12-30T19:49:23.478790Z","shell.execute_reply":"2025-12-30T19:49:23.880409Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\n\nidx = np.arange(len(train))\ntrain_idx, val_idx = train_test_split(idx, test_size=0.1, random_state=42)\n\nclass MTDataset(Dataset):\n    def __init__(self, src, tgt, idxs):\n        self.src = [src[i] for i in idxs]\n        self.tgt = [tgt[i] for i in idxs]\n    def __len__(self): return len(self.src)\n    def __getitem__(self, i): return self.src[i], self.tgt[i]\n\ndef collate_fn(batch, max_src=512, max_tgt=256):\n    src, tgt = zip(*batch)\n    src = [s[:max_src] for s in src]\n    tgt = [t[:max_tgt] for t in tgt]\n    src_len = max(len(s) for s in src)\n    tgt_len = max(len(t) for t in tgt)\n\n    src_pad = torch.full((len(src), src_len), PAD_ID, dtype=torch.long)\n    tgt_pad = torch.full((len(tgt), tgt_len), PAD_ID, dtype=torch.long)\n    for i, (s, t) in enumerate(zip(src, tgt)):\n        src_pad[i, :len(s)] = torch.tensor(s)\n        tgt_pad[i, :len(t)] = torch.tensor(t)\n    return src_pad, tgt_pad\n\ntrain_loader = DataLoader(MTDataset(src_ids, tgt_ids, train_idx), batch_size=32, shuffle=True,  collate_fn=collate_fn)\nval_loader   = DataLoader(MTDataset(src_ids, tgt_ids, val_idx),   batch_size=32, shuffle=False, collate_fn=collate_fn)\n\nprint(\" loaders ready | train batches:\", len(train_loader), \"val batches:\", len(val_loader))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T19:54:00.685629Z","iopub.execute_input":"2025-12-30T19:54:00.686011Z","iopub.status.idle":"2025-12-30T19:54:00.703355Z","shell.execute_reply.started":"2025-12-30T19:54:00.685979Z","shell.execute_reply":"2025-12-30T19:54:00.701552Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn as nn\nimport math\n\nVOCAB = sp.get_piece_size()\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=4096, dropout=0.1):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout)\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n    def forward(self, x):\n        return self.dropout(x + self.pe[:, :x.size(1)])\n\nclass TinyTransformerMT(nn.Module):\n    def __init__(self, vocab_size, d_model=256, nhead=4, layers=3, dim_ff=1024, dropout=0.1):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, d_model)\n        self.pos = PositionalEncoding(d_model, dropout=dropout)\n        self.tf  = nn.Transformer(\n            d_model=d_model, nhead=nhead,\n            num_encoder_layers=layers, num_decoder_layers=layers,\n            dim_feedforward=dim_ff, dropout=dropout,\n            batch_first=True\n        )\n        self.out = nn.Linear(d_model, vocab_size)\n\n    def forward(self, src_ids, tgt_in_ids):\n        src = self.pos(self.emb(src_ids))\n        tgt = self.pos(self.emb(tgt_in_ids))\n        T = tgt_in_ids.size(1)\n        tgt_mask = torch.triu(torch.ones(T, T, device=src.device), diagonal=1).bool()\n        h = self.tf(src, tgt, tgt_mask=tgt_mask)\n        return self.out(h)\n\nmodel = TinyTransformerMT(VOCAB).to(device)\nprint(\" model ready on\", device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T19:54:17.226356Z","iopub.execute_input":"2025-12-30T19:54:17.227102Z","iopub.status.idle":"2025-12-30T19:54:17.462117Z","shell.execute_reply.started":"2025-12-30T19:54:17.227063Z","shell.execute_reply":"2025-12-30T19:54:17.461098Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.optim as optim\nimport torch.nn.functional as F\n\noptimizer = optim.AdamW(model.parameters(), lr=3e-4)\n\ndef loss_label_smoothing(logits, target, ignore_index=PAD_ID, eps=0.1):\n    # logits: (B,T,V), target: (B,T)\n    V = logits.size(-1)\n    logits = logits.reshape(-1, V)\n    target = target.reshape(-1)\n\n    mask = target != ignore_index\n    logits = logits[mask]\n    target = target[mask]\n\n    log_probs = F.log_softmax(logits, dim=-1)\n    nll = -log_probs.gather(1, target.unsqueeze(1)).squeeze(1)\n    smooth = -log_probs.mean(dim=-1)\n    return ((1 - eps) * nll + eps * smooth).mean()\n\ndef run_epoch(loader, train_mode=True, eps=0.1):\n    model.train(train_mode)\n    total = 0.0\n    for src, tgt in loader:\n        src, tgt = src.to(device), tgt.to(device)\n        tgt_in, tgt_out = tgt[:, :-1], tgt[:, 1:]\n\n        logits = model(src, tgt_in)\n        loss = loss_label_smoothing(logits, tgt_out, eps=eps)\n\n        if train_mode:\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n\n        total += loss.item()\n    return total / len(loader)\n\nfor ep in range(3):\n    tr = run_epoch(train_loader, True, eps=0.1)\n    va = run_epoch(val_loader, False, eps=0.1)\n    print(f\"Epoch {ep+1}: train_loss={tr:.4f} val_loss={va:.4f}\")\n\nprint(\" training done\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T20:06:33.166838Z","iopub.execute_input":"2025-12-30T20:06:33.168620Z","iopub.status.idle":"2025-12-30T20:35:26.895341Z","shell.execute_reply.started":"2025-12-30T20:06:33.168566Z","shell.execute_reply":"2025-12-30T20:35:26.893182Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\n\n# Reload train data\ntrain = pd.read_csv(\"/kaggle/input/deep-past-initiative-machine-translation/train.csv\")\n\n# Recreate cleaned columns (must match earlier)\nsub_map = str.maketrans(\"₀₁₂₃₄₅₆₇₈₉\", \"0123456789\")\n\ndef clean_transliteration(t):\n    t = str(t).translate(sub_map)\n    t = re.sub(r\"\\(d\\)\", \"D_\", t)\n    t = re.sub(r\"\\s+\", \" \", t).strip()\n    return t\n\ndef clean_translation(t):\n    t = str(t).replace(\"“\", '\"').replace(\"”\", '\"').replace(\"’\", \"'\")\n    t = re.sub(r\"\\s+\", \" \", t).strip()\n    return t\n\ntrain[\"src\"] = train[\"transliteration\"].apply(clean_transliteration)\ntrain[\"tgt\"] = train[\"translation\"].apply(clean_translation)\n\nprint(\" train reloaded:\", train.shape)\nprint(train[[\"src\", \"tgt\"]].head(1))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T22:40:43.358125Z","iopub.execute_input":"2025-12-30T22:40:43.359076Z","iopub.status.idle":"2025-12-30T22:40:43.651306Z","shell.execute_reply.started":"2025-12-30T22:40:43.359042Z","shell.execute_reply":"2025-12-30T22:40:43.650390Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def encode_with_sp(text):\n    ids = sp.encode(str(text), out_type=int)\n    return [sp.bos_id()] + ids + [sp.eos_id()]\n\nprint(\" encode_with_sp defined\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T22:42:02.743418Z","iopub.execute_input":"2025-12-30T22:42:02.744165Z","iopub.status.idle":"2025-12-30T22:42:02.748833Z","shell.execute_reply.started":"2025-12-30T22:42:02.744135Z","shell.execute_reply":"2025-12-30T22:42:02.748056Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\nimport os, re, pandas as pd, numpy as np, torch, sentencepiece as spm\n\n# ---------- 1. Load data ----------\ntrain = pd.read_csv(\"/kaggle/input/deep-past-initiative-machine-translation/train.csv\")\n\nsub_map = str.maketrans(\"₀₁₂₃₄₅₆₇₈₉\", \"0123456789\")\n\ndef clean_transliteration(t):\n    t = str(t).translate(sub_map)\n    t = re.sub(r\"\\(d\\)\", \"D_\", t)\n    t = re.sub(r\"\\s+\", \" \", t).strip()\n    return t\n\ndef clean_translation(t):\n    t = str(t).replace(\"“\", '\"').replace(\"”\", '\"').replace(\"’\", \"'\")\n    t = re.sub(r\"\\s+\", \" \", t).strip()\n    return t\n\ntrain[\"src\"] = train[\"transliteration\"].apply(clean_transliteration)\ntrain[\"tgt\"] = train[\"translation\"].apply(clean_translation)\n\n# ---------- 2. Train SentencePiece ----------\nos.makedirs(\"/kaggle/working/spm\", exist_ok=True)\n\nsrc_path   = \"/kaggle/working/spm/src.txt\"\ntgt_path   = \"/kaggle/working/spm/tgt.txt\"\njoint_path = \"/kaggle/working/spm/joint.txt\"\n\nwith open(src_path, \"w\", encoding=\"utf-8\") as f:\n    f.write(\"\\n\".join(train[\"src\"]))\n\nwith open(tgt_path, \"w\", encoding=\"utf-8\") as f:\n    f.write(\"\\n\".join(train[\"tgt\"]))\n\nwith open(joint_path, \"w\", encoding=\"utf-8\") as f:\n    f.write(open(src_path).read() + \"\\n\" + open(tgt_path).read())\n\nspm.SentencePieceTrainer.train(\n    input=joint_path,\n    model_prefix=\"/kaggle/working/spm/oa_joint_unigram\",\n    vocab_size=5000,\n    model_type=\"unigram\",\n    character_coverage=1.0,\n    user_defined_symbols=[\"<gap>\", \"<big_gap>\", \"D_\", \"<morph>\"],\n    hard_vocab_limit=False\n)\n\nsp = spm.SentencePieceProcessor()\nsp.load(\"/kaggle/working/spm/oa_joint_unigram.model\")\n\n# ---------- 3. Device + PAD ----------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nPAD_ID = sp.eos_id()\n\n# ---------- 4. Encoder ----------\ndef encode_with_sp(text):\n    ids = sp.encode(str(text), out_type=int)\n    return [sp.bos_id()] + ids + [sp.eos_id()]\n\n# ---------- 5. Encode dataset ----------\nsrc_ids = [encode_with_sp(s) for s in train[\"src\"]]\ntgt_ids = [encode_with_sp(s) for s in train[\"tgt\"]]\n\nprint(\" BOOTSTRAP COMPLETE\")\nprint(\"Train size:\", train.shape)\nprint(\"Vocab size:\", sp.get_piece_size())\nprint(\"Encoded samples:\", len(src_ids), len(tgt_ids))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T22:43:20.670656Z","iopub.execute_input":"2025-12-30T22:43:20.671298Z","iopub.status.idle":"2025-12-30T22:43:22.198375Z","shell.execute_reply.started":"2025-12-30T22:43:20.671267Z","shell.execute_reply":"2025-12-30T22:43:22.197431Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport re\n\n# reload train data\ntrain = pd.read_csv(\"/kaggle/input/deep-past-initiative-machine-translation/train.csv\")\n\nsub_map = str.maketrans(\"₀₁₂₃₄₅₆₇₈₉\", \"0123456789\")\n\ndef clean_transliteration(t):\n    t = str(t).translate(sub_map)\n    t = re.sub(r\"\\(d\\)\", \"D_\", t)\n    t = re.sub(r\"\\s+\", \" \", t).strip()\n    return t\n\ndef clean_translation(t):\n    t = str(t).replace(\"“\", '\"').replace(\"”\", '\"').replace(\"’\", \"'\")\n    t = re.sub(r\"\\s+\", \" \", t).strip()\n    return t\n\ntrain[\"src\"] = train[\"transliteration\"].apply(clean_transliteration)\ntrain[\"tgt\"] = train[\"translation\"].apply(clean_translation)\n\nprint(\" train loaded:\", train.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T13:53:03.310830Z","iopub.execute_input":"2026-01-01T13:53:03.311254Z","iopub.status.idle":"2026-01-01T13:53:03.610625Z","shell.execute_reply.started":"2026-01-01T13:53:03.311218Z","shell.execute_reply":"2026-01-01T13:53:03.609855Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"idx = np.arange(len(train))\ntrain_idx, val_idx = train_test_split(idx, test_size=0.1, random_state=42)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T13:54:02.343271Z","iopub.execute_input":"2026-01-01T13:54:02.343638Z","iopub.status.idle":"2026-01-01T13:54:02.349004Z","shell.execute_reply.started":"2026-01-01T13:54:02.343609Z","shell.execute_reply":"2026-01-01T13:54:02.348306Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T13:55:34.851608Z","iopub.execute_input":"2026-01-01T13:55:34.852006Z","iopub.status.idle":"2026-01-01T13:55:34.856514Z","shell.execute_reply.started":"2026-01-01T13:55:34.851975Z","shell.execute_reply":"2026-01-01T13:55:34.855677Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def encode_with_sp(text):\n    ids = sp.encode(str(text), out_type=int)\n    return [sp.bos_id()] + ids + [sp.eos_id()]\n\nprint(\" encode_with_sp defined\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T13:56:41.227206Z","iopub.execute_input":"2026-01-01T13:56:41.227923Z","iopub.status.idle":"2026-01-01T13:56:41.233205Z","shell.execute_reply.started":"2026-01-01T13:56:41.227886Z","shell.execute_reply":"2026-01-01T13:56:41.232309Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\nimport os, re, pandas as pd, numpy as np, torch, sentencepiece as spm\n\n# 1) Load data\ntrain = pd.read_csv(\"/kaggle/input/deep-past-initiative-machine-translation/train.csv\")\n\nsub_map = str.maketrans(\"₀₁₂₃₄₅₆₇₈₉\", \"0123456789\")\n\ndef clean_transliteration(t):\n    t = str(t).translate(sub_map)\n    t = re.sub(r\"\\(d\\)\", \"D_\", t)\n    t = re.sub(r\"\\s+\", \" \", t).strip()\n    return t\n\ndef clean_translation(t):\n    t = str(t).replace(\"“\", '\"').replace(\"”\", '\"').replace(\"’\", \"'\")\n    t = re.sub(r\"\\s+\", \" \", t).strip()\n    return t\n\ntrain[\"src\"] = train[\"transliteration\"].apply(clean_transliteration)\ntrain[\"tgt\"] = train[\"translation\"].apply(clean_translation)\n\n# 2) SentencePiece\nos.makedirs(\"/kaggle/working/spm\", exist_ok=True)\n\nwith open(\"/kaggle/working/spm/src.txt\", \"w\") as f:\n    f.write(\"\\n\".join(train[\"src\"]))\nwith open(\"/kaggle/working/spm/tgt.txt\", \"w\") as f:\n    f.write(\"\\n\".join(train[\"tgt\"]))\nwith open(\"/kaggle/working/spm/joint.txt\", \"w\") as f:\n    f.write(open(\"/kaggle/working/spm/src.txt\").read() + \"\\n\" +\n            open(\"/kaggle/working/spm/tgt.txt\").read())\n\nspm.SentencePieceTrainer.train(\n    input=\"/kaggle/working/spm/joint.txt\",\n    model_prefix=\"/kaggle/working/spm/oa\",\n    vocab_size=5000,\n    model_type=\"unigram\",\n    character_coverage=1.0,\n    hard_vocab_limit=False\n)\n\nsp = spm.SentencePieceProcessor()\nsp.load(\"/kaggle/working/spm/oa.model\")\n\n# 3) Encoder\ndef encode_with_sp(text):\n    ids = sp.encode(str(text), out_type=int)\n    return [sp.bos_id()] + ids + [sp.eos_id()]\n\n# 4) Encode data\nsrc_ids = [encode_with_sp(s) for s in train[\"src\"]]\ntgt_ids = [encode_with_sp(s) for s in train[\"tgt\"]]\n\nPAD_ID = sp.eos_id()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(\" EVERYTHING READY\")\nprint(\"train:\", train.shape)\nprint(\"vocab:\", sp.get_piece_size())\nprint(\"encoded:\", len(src_ids), len(tgt_ids))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T13:58:34.218441Z","iopub.execute_input":"2026-01-01T13:58:34.218772Z","iopub.status.idle":"2026-01-01T13:58:35.694749Z","shell.execute_reply.started":"2026-01-01T13:58:34.218743Z","shell.execute_reply":"2026-01-01T13:58:35.693973Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd, numpy as np, torch, re\nfrom nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\nfrom nltk.translate.chrf_score import corpus_chrf\n\n# -----------------------\n# 1) Build val_loader (if missing)\n# -----------------------\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\n\nidx = np.arange(len(train))\ntrain_idx, val_idx = train_test_split(idx, test_size=0.1, random_state=42)\n\nclass MTDataset(Dataset):\n    def __init__(self, src, tgt, idxs):\n        self.src = [src[i] for i in idxs]\n        self.tgt = [tgt[i] for i in idxs]\n    def __len__(self): return len(self.src)\n    def __getitem__(self, i): return self.src[i], self.tgt[i]\n\ndef collate_fn(batch, max_src=512, max_tgt=256):\n    src, tgt = zip(*batch)\n    src = [s[:max_src] for s in src]\n    tgt = [t[:max_tgt] for t in tgt]\n    src_len = max(len(s) for s in src)\n    tgt_len = max(len(t) for t in tgt)\n\n    src_pad = torch.full((len(src), src_len), PAD_ID, dtype=torch.long)\n    tgt_pad = torch.full((len(tgt), tgt_len), PAD_ID, dtype=torch.long)\n    for i, (s, t) in enumerate(zip(src, tgt)):\n        src_pad[i, :len(s)] = torch.tensor(s)\n        tgt_pad[i, :len(t)] = torch.tensor(t)\n    return src_pad, tgt_pad\n\nval_loader = DataLoader(MTDataset(src_ids, tgt_ids, val_idx), batch_size=32, shuffle=False, collate_fn=collate_fn)\nprint(\" val_loader batches:\", len(val_loader))\n\n# -----------------------\n# 2) Safety check: model must exist\n# -----------------------\ntry:\n    model\nexcept NameError:\n    raise NameError(\" model not defined. Please run your MODEL + TRAINING cell first, then run this cell again.\")\n\n# -----------------------\n# 3) Lexicon postprocess (optional but helpful)\n# -----------------------\nlex = pd.read_csv(\"/kaggle/input/deep-past-initiative-machine-translation/OA_Lexicon_eBL.csv\")\nlex.columns = [c.lower() for c in lex.columns]\nsrc_col = next((c for c in lex.columns if any(k in c for k in [\"translit\",\"lemma\",\"form\",\"akk\"])), None)\ntgt_col = next((c for c in lex.columns if any(k in c for k in [\"normal\",\"english\",\"name\",\"norm\"])), None)\n\nlex_map = {}\nif src_col and tgt_col:\n    for s, t in lex[[src_col, tgt_col]].dropna().values:\n        s, t = str(s).strip(), str(t).strip()\n        if len(s) >= 2 and len(t) >= 2:\n            lex_map[s] = t\n\nLEX_K = 2000\nlex_keys = sorted(list(lex_map.keys()), key=len, reverse=True)[:LEX_K]\nlex_re = re.compile(r\"\\b(\" + \"|\".join(map(re.escape, lex_keys)) + r\")\\b\") if lex_keys else None\n\ndef lexicon_postprocess(text: str) -> str:\n    if not lex_re: return text\n    return lex_re.sub(lambda m: lex_map.get(m.group(1), m.group(1)), text)\n\n# -----------------------\n# 4) Beam decode\n# -----------------------\n@torch.no_grad()\ndef beam_decode_batch(src, beam_size=5, max_len=160, length_penalty=0.8):\n    model.eval()\n    src = src.to(device)\n    beams = [(torch.full((src.size(0), 1), sp.bos_id(), device=device), torch.zeros(src.size(0), device=device))]\n\n    for _ in range(max_len):\n        new_beams = []\n        for seq, score in beams:\n            logits = model(src, seq)\n            logp = torch.log_softmax(logits[:, -1], dim=-1)\n            topk = torch.topk(logp, beam_size, dim=-1)\n\n            for k in range(beam_size):\n                next_id = topk.indices[:, k:k+1]\n                next_score = score + topk.values[:, k]\n                new_seq = torch.cat([seq, next_id], dim=1)\n                new_beams.append((new_seq, next_score))\n\n        def rank(item):\n            seq, sc = item\n            lp = (seq.size(1) ** length_penalty)\n            return (sc / lp).mean().item()\n\n        new_beams.sort(key=rank, reverse=True)\n        beams = new_beams[:beam_size]\n\n        if all((b[0][:, -1] == sp.eos_id()).all() for b in beams):\n            break\n\n    return beams[0][0]\n\ndef decode_ids(ids):\n    # remove special tokens safely\n    ids = [i for i in ids if i not in (sp.bos_id(),)]\n    if sp.eos_id() in ids:\n        ids = ids[:ids.index(sp.eos_id())]\n    return sp.decode(ids)\n\n# -----------------------\n# 5) Metrics + eval_boosted\n# -----------------------\ndef compute_metrics(preds, refs):\n    refs_tok = [[r.split()] for r in refs]\n    preds_tok = [p.split() for p in preds]\n    bleu = corpus_bleu(refs_tok, preds_tok, smoothing_function=SmoothingFunction().method4) * 100\n    chrf = corpus_chrf(refs, preds, beta=2) * 100\n    final = (bleu * chrf) ** 0.5\n    return bleu, chrf, final\n\n@torch.no_grad()\ndef eval_boosted(max_batches=20, beam=5, lp=0.8):\n    preds, refs = [], []\n    seen = 0\n    for b, (src, tgt) in enumerate(val_loader):\n        if b >= max_batches: break\n        out_ids = beam_decode_batch(src, beam_size=beam, length_penalty=lp)\n        for i in range(out_ids.size(0)):\n            pred = decode_ids(out_ids[i].tolist())\n            pred = lexicon_postprocess(pred)\n            preds.append(pred)\n            refs.append(train[\"tgt\"].iloc[val_idx[seen]])\n            seen += 1\n\n    bleu, chrf, final = compute_metrics(preds, refs)\n    print(f\"[EVAL] beam={beam} lp={lp} | BLEU={bleu:.2f} chrF++={chrf:.2f} FINAL={final:.2f}\")\n    print(\"Sample pred:\", preds[0][:200])\n    print(\"Sample ref :\", refs[0][:200])\n    return bleu, chrf, final\n\n# -----------------------\n# 6) RUN NOW\n# -----------------------\neval_boosted(max_batches=20, beam=5, lp=0.8)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T14:03:11.651110Z","iopub.execute_input":"2026-01-01T14:03:11.651995Z","iopub.status.idle":"2026-01-01T14:03:12.478876Z","shell.execute_reply.started":"2026-01-01T14:03:11.651960Z","shell.execute_reply":"2026-01-01T14:03:12.477905Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\n\n# Safety: required variables should exist\nassert \"train\" in globals(), \"train missing (reload train).\"\nassert \"src_ids\" in globals() and \"tgt_ids\" in globals(), \"src_ids/tgt_ids missing (run bootstrap encoding).\"\nassert \"PAD_ID\" in globals(), \"PAD_ID missing.\"\nassert len(src_ids) == len(tgt_ids) == len(train), \"Mismatch in lengths!\"\n\n# Split\nidx = np.arange(len(train))\ntrain_idx, val_idx = train_test_split(idx, test_size=0.1, random_state=42)\n\nclass MTDataset(Dataset):\n    def __init__(self, src, tgt, idxs):\n        self.src = [src[i] for i in idxs]\n        self.tgt = [tgt[i] for i in idxs]\n    def __len__(self): return len(self.src)\n    def __getitem__(self, i): return self.src[i], self.tgt[i]\n\ndef collate_fn(batch, max_src=512, max_tgt=256):\n    src, tgt = zip(*batch)\n    src = [s[:max_src] for s in src]\n    tgt = [t[:max_tgt] for t in tgt]\n\n    src_len = max(len(s) for s in src)\n    tgt_len = max(len(t) for t in tgt)\n\n    src_pad = torch.full((len(src), src_len), PAD_ID, dtype=torch.long)\n    tgt_pad = torch.full((len(tgt), tgt_len), PAD_ID, dtype=torch.long)\n\n    for i, (s, t) in enumerate(zip(src, tgt)):\n        src_pad[i, :len(s)] = torch.tensor(s)\n        tgt_pad[i, :len(t)] = torch.tensor(t)\n\n    return src_pad, tgt_pad\n\ntrain_loader = DataLoader(\n    MTDataset(src_ids, tgt_ids, train_idx),\n    batch_size=32, shuffle=True, collate_fn=collate_fn\n)\n\nval_loader = DataLoader(\n    MTDataset(src_ids, tgt_ids, val_idx),\n    batch_size=32, shuffle=False, collate_fn=collate_fn\n)\n\nprint(\" loaders ready!\")\nprint(\"train batches:\", len(train_loader))\nprint(\"val batches  :\", len(val_loader))\nprint(\"train samples:\", len(train_idx), \"val samples:\", len(val_idx))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T14:09:06.030002Z","iopub.execute_input":"2026-01-01T14:09:06.030963Z","iopub.status.idle":"2026-01-01T14:09:06.045017Z","shell.execute_reply.started":"2026-01-01T14:09:06.030925Z","shell.execute_reply":"2026-01-01T14:09:06.044158Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===============================\n# MASTER BOOTSTRAP (RUN ONCE)\n# ===============================\n\nimport os, re, math\nimport pandas as pd\nimport numpy as np\nimport torch\nimport sentencepiece as spm\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\n\n# ---------- 1. Load data ----------\ntrain = pd.read_csv(\"/kaggle/input/deep-past-initiative-machine-translation/train.csv\")\n\nsub_map = str.maketrans(\"₀₁₂₃₄₅₆₇₈₉\", \"0123456789\")\n\ndef clean_transliteration(t):\n    t = str(t).translate(sub_map)\n    t = re.sub(r\"\\(d\\)\", \"D_\", t)\n    t = re.sub(r\"\\s+\", \" \", t).strip()\n    return t\n\ndef clean_translation(t):\n    t = str(t).replace(\"“\", '\"').replace(\"”\", '\"').replace(\"’\", \"'\")\n    t = re.sub(r\"\\s+\", \" \", t).strip()\n    return t\n\ntrain[\"src\"] = train[\"transliteration\"].apply(clean_transliteration)\ntrain[\"tgt\"] = train[\"translation\"].apply(clean_translation)\n\n# ---------- 2. SentencePiece ----------\nos.makedirs(\"/kaggle/working/spm\", exist_ok=True)\n\nwith open(\"/kaggle/working/spm/src.txt\", \"w\") as f:\n    f.write(\"\\n\".join(train[\"src\"]))\nwith open(\"/kaggle/working/spm/tgt.txt\", \"w\") as f:\n    f.write(\"\\n\".join(train[\"tgt\"]))\nwith open(\"/kaggle/working/spm/joint.txt\", \"w\") as f:\n    f.write(open(\"/kaggle/working/spm/src.txt\").read() + \"\\n\" +\n            open(\"/kaggle/working/spm/tgt.txt\").read())\n\nspm.SentencePieceTrainer.train(\n    input=\"/kaggle/working/spm/joint.txt\",\n    model_prefix=\"/kaggle/working/spm/oa\",\n    vocab_size=5000,\n    model_type=\"unigram\",\n    character_coverage=1.0,\n    hard_vocab_limit=False\n)\n\nsp = spm.SentencePieceProcessor()\nsp.load(\"/kaggle/working/spm/oa.model\")\n\n# ---------- 3. Encode ----------\ndef encode_with_sp(text):\n    ids = sp.encode(str(text), out_type=int)\n    return [sp.bos_id()] + ids + [sp.eos_id()]\n\nsrc_ids = [encode_with_sp(s) for s in train[\"src\"]]\ntgt_ids = [encode_with_sp(s) for s in train[\"tgt\"]]\n\nPAD_ID = sp.eos_id()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ---------- 4. Loaders ----------\nidx = np.arange(len(train))\ntrain_idx, val_idx = train_test_split(idx, test_size=0.1, random_state=42)\n\nclass MTDataset(Dataset):\n    def __init__(self, src, tgt, idxs):\n        self.src = [src[i] for i in idxs]\n        self.tgt = [tgt[i] for i in idxs]\n    def __len__(self): return len(self.src)\n    def __getitem__(self, i): return self.src[i], self.tgt[i]\n\ndef collate_fn(batch, max_src=512, max_tgt=256):\n    src, tgt = zip(*batch)\n    src = [s[:max_src] for s in src]\n    tgt = [t[:max_tgt] for t in tgt]\n\n    src_len = max(len(s) for s in src)\n    tgt_len = max(len(t) for t in tgt)\n\n    src_pad = torch.full((len(src), src_len), PAD_ID, dtype=torch.long)\n    tgt_pad = torch.full((len(tgt), tgt_len), PAD_ID, dtype=torch.long)\n\n    for i, (s, t) in enumerate(zip(src, tgt)):\n        src_pad[i, :len(s)] = torch.tensor(s)\n        tgt_pad[i, :len(t)] = torch.tensor(t)\n\n    return src_pad, tgt_pad\n\ntrain_loader = DataLoader(\n    MTDataset(src_ids, tgt_ids, train_idx),\n    batch_size=32, shuffle=True, collate_fn=collate_fn\n)\n\nval_loader = DataLoader(\n    MTDataset(src_ids, tgt_ids, val_idx),\n    batch_size=32, shuffle=False, collate_fn=collate_fn\n)\n\nprint(\" BOOTSTRAP COMPLETE\")\nprint(\"train:\", train.shape)\nprint(\"vocab:\", sp.get_piece_size())\nprint(\"encoded:\", len(src_ids))\nprint(\"train_loader:\", len(train_loader), \"val_loader:\", len(val_loader))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T18:18:21.590386Z","iopub.execute_input":"2026-01-01T18:18:21.592184Z","iopub.status.idle":"2026-01-01T18:18:23.278170Z","shell.execute_reply.started":"2026-01-01T18:18:21.592132Z","shell.execute_reply":"2026-01-01T18:18:23.277096Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch, torch.nn as nn, torch.nn.functional as F\nimport math\nfrom nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\nfrom nltk.translate.chrf_score import corpus_chrf\n\n# -----------------------\n# 1) Model\n# -----------------------\nVOCAB = sp.get_piece_size()\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=4096, dropout=0.1):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout)\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe.unsqueeze(0))\n    def forward(self, x):\n        return self.dropout(x + self.pe[:, :x.size(1)])\n\nclass TinyTransformerMT(nn.Module):\n    def __init__(self, vocab_size, d_model=256, nhead=4, layers=3, dim_ff=1024, dropout=0.1):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, d_model)\n        self.pos = PositionalEncoding(d_model, dropout=dropout)\n        self.tf  = nn.Transformer(\n            d_model=d_model, nhead=nhead,\n            num_encoder_layers=layers, num_decoder_layers=layers,\n            dim_feedforward=dim_ff, dropout=dropout,\n            batch_first=True\n        )\n        self.out = nn.Linear(d_model, vocab_size)\n    def forward(self, src_ids, tgt_in_ids):\n        src = self.pos(self.emb(src_ids))\n        tgt = self.pos(self.emb(tgt_in_ids))\n        T = tgt_in_ids.size(1)\n        tgt_mask = torch.triu(torch.ones(T, T, device=src.device), diagonal=1).bool()\n        h = self.tf(src, tgt, tgt_mask=tgt_mask)\n        return self.out(h)\n\nmodel = TinyTransformerMT(VOCAB).to(device)\n\n# -----------------------\n# 2) Loss + optimizer\n# -----------------------\nopt = torch.optim.AdamW(model.parameters(), lr=3e-4)\n\ndef loss_label_smoothing(logits, target, ignore_index=PAD_ID, eps=0.1):\n    V = logits.size(-1)\n    logits = logits.reshape(-1, V)\n    target = target.reshape(-1)\n    mask = target != ignore_index\n    logits = logits[mask]\n    target = target[mask]\n    log_probs = F.log_softmax(logits, dim=-1)\n    nll = -log_probs.gather(1, target.unsqueeze(1)).squeeze(1)\n    smooth = -log_probs.mean(dim=-1)\n    return ((1 - eps) * nll + eps * smooth).mean()\n\ndef run_epoch(loader, train_mode=True, eps=0.1):\n    model.train(train_mode)\n    total = 0.0\n    for src, tgt in loader:\n        src, tgt = src.to(device), tgt.to(device)\n        tgt_in, tgt_out = tgt[:, :-1], tgt[:, 1:]\n        logits = model(src, tgt_in)\n        loss = loss_label_smoothing(logits, tgt_out, eps=eps)\n\n        if train_mode:\n            opt.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            opt.step()\n\n        total += loss.item()\n    return total / len(loader)\n\n# -----------------------\n# 3) Train 2 epochs\n# -----------------------\nfor ep in range(2):\n    tr = run_epoch(train_loader, True, eps=0.1)\n    va = run_epoch(val_loader, False, eps=0.1)\n    print(f\"Epoch {ep+1}: train_loss={tr:.4f} val_loss={va:.4f}\")\n\nprint(\" model trained\")\n\n# -----------------------\n# 4) Beam decode + eval\n# -----------------------\n@torch.no_grad()\ndef beam_decode_batch(src, beam_size=5, max_len=160, length_penalty=0.8):\n    model.eval()\n    src = src.to(device)\n    beams = [(torch.full((src.size(0), 1), sp.bos_id(), device=device), torch.zeros(src.size(0), device=device))]\n\n    for _ in range(max_len):\n        new_beams = []\n        for seq, score in beams:\n            logits = model(src, seq)\n            logp = torch.log_softmax(logits[:, -1], dim=-1)\n            topk = torch.topk(logp, beam_size, dim=-1)\n            for k in range(beam_size):\n                next_id = topk.indices[:, k:k+1]\n                next_score = score + topk.values[:, k]\n                new_seq = torch.cat([seq, next_id], dim=1)\n                new_beams.append((new_seq, next_score))\n\n        def rank(item):\n            seq, sc = item\n            lp = (seq.size(1) ** length_penalty)\n            return (sc / lp).mean().item()\n\n        new_beams.sort(key=rank, reverse=True)\n        beams = new_beams[:beam_size]\n\n        if all((b[0][:, -1] == sp.eos_id()).all() for b in beams):\n            break\n\n    return beams[0][0]\n\ndef decode_ids(ids):\n    ids = [i for i in ids if i != sp.bos_id()]\n    if sp.eos_id() in ids:\n        ids = ids[:ids.index(sp.eos_id())]\n    return sp.decode(ids)\n\ndef compute_metrics(preds, refs):\n    refs_tok = [[r.split()] for r in refs]\n    preds_tok = [p.split() for p in preds]\n    bleu = corpus_bleu(refs_tok, preds_tok, smoothing_function=SmoothingFunction().method4) * 100\n    chrf = corpus_chrf(refs, preds, beta=2) * 100\n    final = (bleu * chrf) ** 0.5\n    return bleu, chrf, final\n\n@torch.no_grad()\ndef quick_eval(max_batches=10, beam=5, lp=0.8):\n    preds, refs = [], []\n    seen = 0\n    for b, (src, tgt) in enumerate(val_loader):\n        if b >= max_batches: break\n        out_ids = beam_decode_batch(src, beam_size=beam, length_penalty=lp)\n        for i in range(out_ids.size(0)):\n            preds.append(decode_ids(out_ids[i].tolist()))\n            refs.append(train[\"tgt\"].iloc[val_idx[seen]])\n            seen += 1\n\n    bleu, chrf, final = compute_metrics(preds, refs)\n    print(f\"[EVAL] beam={beam} lp={lp} | BLEU={bleu:.2f} chrF++={chrf:.2f} FINAL={final:.2f}\")\n    print(\"pred:\", preds[0][:160])\n    print(\"ref :\", refs[0][:160])\n    return bleu, chrf, final\n\nquick_eval(max_batches=10, beam=5, lp=0.8)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T18:24:31.706665Z","iopub.execute_input":"2026-01-01T18:24:31.707141Z","iopub.status.idle":"2026-01-01T20:12:02.155781Z","shell.execute_reply.started":"2026-01-01T18:24:31.707109Z","shell.execute_reply":"2026-01-01T20:12:02.154388Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------\n# Build submission.csv\n# -----------------------\nimport pandas as pd\nimport torch\n\n# 1) Load test\ntest = pd.read_csv(\"/kaggle/input/deep-past-initiative-machine-translation/test.csv\")\ntest[\"src\"] = test[\"transliteration\"].apply(clean_transliteration)\n\n# 2) Helper: predict in batches using beam\n@torch.no_grad()\ndef predict_texts_beam(texts, batch_size=32, beam=5, lp=0.8):\n    outs = []\n    for i in range(0, len(texts), batch_size):\n        chunk = texts[i:i+batch_size]\n        enc = [encode_with_sp(s) for s in chunk]\n        maxlen = max(len(x) for x in enc)\n        src = torch.full((len(enc), maxlen), PAD_ID, dtype=torch.long)\n        for j, x in enumerate(enc):\n            src[j, :len(x)] = torch.tensor(x)\n        out_ids = beam_decode_batch(src, beam_size=beam, length_penalty=lp)\n        for k in range(out_ids.size(0)):\n            outs.append(decode_ids(out_ids[k].tolist()))\n    return outs\n\npreds = predict_texts_beam(test[\"src\"].tolist(), batch_size=32, beam=5, lp=0.8)\n\n# 3) Save submission\nsub = pd.read_csv(\"/kaggle/input/deep-past-initiative-machine-translation/sample_submission.csv\")\nsub[\"translation\"] = preds\nsub.to_csv(\"submission.csv\", index=False)\n\nprint(\" submission.csv created:\", sub.shape)\nsub.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T20:18:43.347553Z","iopub.execute_input":"2026-01-01T20:18:43.347954Z","iopub.status.idle":"2026-01-01T20:19:44.420103Z","shell.execute_reply.started":"2026-01-01T20:18:43.347920Z","shell.execute_reply":"2026-01-01T20:19:44.418877Z"}},"outputs":[],"execution_count":null}]}